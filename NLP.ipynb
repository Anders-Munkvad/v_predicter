{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We want text as text and prices of the currencies.\n",
    "\n",
    "I.e. we want to see if the text has any influence on the prices of the currencies.\n",
    "\n",
    "Problems: Where do we get dataset? Create one yourself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset, e.g. the same as the following (just a random dataset, i.e. a language dataset for finish QA):\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "train_df_translated = pd.read_csv(\"translated_train_data.csv\")\n",
    "\n",
    "validation_df_translated = pd.read_csv(\"translated_validation_data.csv\")\n",
    "\n",
    "# Finnish\n",
    "finnish_train_df = train_df_translated[train_df_translated[\"lang\"] == \"fi\"].reset_index()\n",
    "finnish_val_df = validation_df_translated[validation_df_translated[\"lang\"] == \"fi\"].reset_index()\n",
    "\n",
    "# Choose what we want to be the input, i.e. in this case it will be question and the context column concatinated into the \"all_text\" column:\n",
    "\n",
    "finnish_train_df[\"all_text\"] = finnish_train_df[\"question_en\"] + \" <concat> \" + finnish_train_df[\"context\"]\n",
    "finnish_val_df[\"all_text\"] = finnish_val_df[\"question_en\"] + \" <concat> \" + finnish_val_df[\"context\"]\n",
    "\n",
    "finnish_train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# We need text vectorizer, as the model can not read text. I.e. we create a vector, essentially mapping each word to an integer, so that the model can actually read it.\n",
    "# This is our tokenization in this case, e.g. tokenization could also be unigram, bigram and so on (i think)\n",
    "\n",
    "# NOTE: The following uses IOB tags - we won't need that in our case, atleast i can't see a reason to\n",
    "\n",
    "def create_and_adapt_vectorizer(text_data, max_tokens=20000, max_length=200):\n",
    "    vectorizer = TextVectorization(\n",
    "        max_tokens=max_tokens,\n",
    "        output_mode='int',\n",
    "        output_sequence_length=max_length\n",
    "    )\n",
    "    vectorizer.adapt(text_data.values)\n",
    "    return vectorizer\n",
    "\n",
    "# Adapt vectorizer for Finnish dataset\n",
    "finnish_vectorizer = create_and_adapt_vectorizer(finnish_train_df[\"all_text\"])\n",
    "\n",
    "#labels = finnish_train_df[\"zipped_tokens_labels\"] eller er det iob_labels?\n",
    "\n",
    "# Sequences = what we want as input, in this case it will be the \"all_text\" column\n",
    "# Labels = what we are trying to predict, in this case it will be the \"iob_labels\" (these have not been generated in this example.)\n",
    "def preprocess_data_with_vectorizer(df, vectorizer):\n",
    "    sequences = vectorizer(df['all_text'].values)  # Tokenize and transform text data\n",
    "    labels = np.array(df['iob_labels'])  # Convert labels to numpy array - use iob_labels, we don't need to consider the tokens\n",
    "    return sequences, labels\n",
    "\n",
    "# Preprocess training and validation data:\n",
    "finnish_train_padded, finnish_train_labels = preprocess_data_with_vectorizer(finnish_train_df, finnish_vectorizer)\n",
    "finnish_val_padded, finnish_val_labels = preprocess_data_with_vectorizer(finnish_val_df, finnish_vectorizer)\n",
    "\n",
    "# Encode IOB labels ('O', 'B', 'I') to numerical form, since it cannot read O, I, B\n",
    "iob_tag_to_index = {'O': 0, 'B': 1, 'I': 2}\n",
    "\n",
    "def encode_iob_labels(iob_labels, max_length=200):\n",
    "    # First, convert the IOB tags to their respective indices\n",
    "    encoded_labels = [[iob_tag_to_index[tag] for tag in label_sequence] for label_sequence in iob_labels]\n",
    "    \n",
    "    # Now pad the sequences to ensure they all have the same length - if they don't have the same length, we get an error - padding shouldnt't really affect anything\n",
    "    padded_encoded_labels = pad_sequences(encoded_labels, maxlen=max_length, padding='post', value=iob_tag_to_index['O'])\n",
    "    \n",
    "    return np.array(padded_encoded_labels)\n",
    "\n",
    "\n",
    "# Encode and pad the labels\n",
    "finnish_train_encoded_labels = encode_iob_labels(finnish_train_labels) # Will just have max_length=200\n",
    "finnish_val_encoded_labels = encode_iob_labels(finnish_val_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "#max_length = 200 # same as vectorizer\n",
    "\n",
    "def build_qa_model(vocab_size, embedding_dim=128, rnn_units=64, output_dim=3, max_length=200):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(max_length,)),  # Input shape should match the vectorizer's output sequence length\n",
    "        layers.Embedding(vocab_size, embedding_dim),\n",
    "        layers.Bidirectional(layers.LSTM(rnn_units, return_sequences=True)),  # Bidirectional LSTM for sequence labeling\n",
    "        layers.TimeDistributed(layers.Dense(output_dim, activation='softmax'))  # Predicts IOB tag for each token\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create and compile the model\n",
    "vocab_size = finnish_vectorizer.vocabulary_size()\n",
    "max_length = 200 # same as vectorizer - idk why i cant just access the attribute\n",
    "qa_model = build_qa_model(vocab_size=vocab_size, max_length=max_length)\n",
    "\n",
    "# Train the model\n",
    "qa_model.fit(finnish_train_padded, finnish_train_encoded_labels, \n",
    "             validation_data=(finnish_val_padded, finnish_val_encoded_labels), \n",
    "             epochs=2, batch_size=32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
